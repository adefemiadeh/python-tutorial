{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __*DATA SCIENCE*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Tools Needed:*__\n",
    "\n",
    "- Google Colab\n",
    "- VScode (Install Extension: Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are standard steps that you have to follow for a data science project. For any project, first, we have to collect the data according to our business needs. The next step is to clean the data like removing values, removing outliers, handling imbalanced datasets,changing categorical variables to numerical values, etc.*\n",
    "\n",
    "*After that training of a model, use various machine learning and deep learning algorithms. Next, is model evaluation using different metrics like recall, f1 score accuracy, etc. Finally, model deployment on the cloud and retrain a model*\n",
    "\n",
    "`Data Collection` --> `Data preparation` --> `Train Model` --> `Analysis/Evaluation` --> `Serve Model` --> `Retrain Model`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### __1. Data Collection__ (*Questions to ask*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What kind of problem are we trying to solve? \n",
    "2. What data sources already exist? \n",
    "3. What privacy concerns are there? \n",
    "4. Is the data public? \n",
    "5. Where should we store the files?\n",
    "\n",
    "__Types of data__\n",
    "\n",
    "1. __Structured Data__: appears in tabulated format (rows and columns style, like what you’d find in\n",
    "an Excel spreadsheet). It contains different types of data, for example numerical, categorical,\n",
    "time series. \n",
    "\n",
    "- __Nominal/categorical__ : One thing or another (mutually exclusive). For example, for car \n",
    "scales, color is a category. A car may be blue but not white. An order does not matter. \n",
    "\n",
    "- __Numerical__: Any continuous value where the difference between them matters. For example, \n",
    "when selling houses, $107,850 is more than $56,400. \n",
    "\n",
    "- __Ordinal__: Data which has order but the distance between values is unknown. \n",
    "\n",
    "    For example, a question such as: how would you rate your health from 1-5? 1 being poor, 5 being healthy.\n",
    " \n",
    "    You can answer 1,2,3,4,5 but the distance between each value doesn’t necessarily mean an answer of 5 is five times as good as an answer of 1.\n",
    "\n",
    "- Time-series: Data across time. For example, the historical sale values of Bulldozers from \n",
    "2012-2018. \n",
    "\n",
    "2. __Unstructured Data__: Data with no rigid structure(images, video, speech, natural\n",
    "language text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __2. Data preparation__ (*Exploratory data analysis(EDA)*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the feature variables (input) and the target variable (output) For example, for predicting heart disease, the feature variables may be a person’s age, weight, average heart rate, and level of physical activity. And the target variable will be whether or not they have a disease \n",
    "\n",
    "2. What kind of do you have? Structured, unstructured, numerical, time series. Are there missing values? Should you remove them or fill them feature imputation. \n",
    "\n",
    "3. Where are the outliers? How many of them are there? Why are they there? Are there any questions you could ask a domain expert about the data? For example, would a heart disease physician be able to shed some light on your heart disease dataset?\n",
    "\n",
    "__Feature Imputation__: filling missing values ( a machine learning model can’t learn\n",
    "on data that’s isn’t there) \n",
    "\n",
    "- imputation: Fill with mean, a median of the column. \n",
    "\n",
    "__Feature Encoding__ (turning values into numbers). A machine learning model requires all values to be numerical \n",
    "\n",
    "- One hot encoding:Turn all unique values into lists of 0’s and 1’s where the target value is 1 and the rest are 0’s. For example, when a car colors green, red blue, a green, a car’s color future would be represented as [1, 0, and 0] and a red one would be [0, 1, and 0]. \n",
    "\n",
    "- Label Encoder: Turn labels into distinct numerical values. For example, if your target variables are different animals, such as dog, cat, bird, these could become 0, 1, and 2, respectively.\n",
    "\n",
    "__Data splitting__\n",
    "1. Training set (usually 70-80% of data): Model learns on this. \n",
    "\n",
    "2. Validation set (usually 10-15% of data): Model hyperparameters are tuned on this \n",
    "\n",
    "3. Test set (usually 10-15% of data): Models’ final performance is evaluated on this. If you have done it right, hopefully, the results on the test set give a good indication of how the model should perform in the real world. Do not use this dataset to tune the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### __3. Train model on data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3 steps: Choose an algorithm, Reduce overfitting with regularization)\n",
    "\n",
    "__Choosing an algorithm__\n",
    "\n",
    "1. __Supervised Algorithms__ – Linear Regression, Logistic Regression, KNN, SVMs, Decision tree and Random forests, AdaBoost/Gradient Boosting Machine(boosting). This can be further devided into:\n",
    "\n",
    "- Classification:\n",
    "\n",
    "- Regression:\n",
    "\n",
    "2. __Unsupervised Algorithms__- Clustering, dimensionality reduction( PCA, Autoencoders, tSNE), An anomaly detection \n",
    "• Underfitting – happens when your model doesn’t perform as well as you’d like on your data.\n",
    "\n",
    "__*Some terms to note:*__\n",
    "\n",
    "- Underfitting– happens model is too simple to capture patterns from he data.\n",
    "\n",
    "- Overfitting– happens when your validation loss starts to increase or when the model performs better on the training set than on the test set. \n",
    "\n",
    "- Regularization: a collection of technologies to prevent/reduce overfitting (e.g. L1, L2, Dropout, Early stopping, Data augmentation, Batch normalization) \n",
    "\n",
    "- Hyperparameter Tuning – run a bunch of experiments with different settings and see which\n",
    "works best "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### __4. Analysis/Evaluation__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "__Evaluation metrics__\n",
    "\n",
    "1. Classification- Accuracy, Precision, Recall, F1, Confusion matrix, Mean average precision \n",
    "(object detection) \n",
    "\n",
    "2. Regression – MSE,MAE,RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
